<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Training a feed-forward Neural Network with Back-propagation and Gradient Descent | Axtya Darius Barbano</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Training a feed-forward Neural Network with Back-propagation and Gradient Descent" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The training of an Artificial Neural Network is dependent on an algorithm called Back-propagation, through which we calculate the effect each weight has on an error function. Each of these ‘effects’ is called a partial derivative, more specifically the partial derivative of the error function with respect to a given weight. A collection of partial derivatives (one for each weight), is called the gradient, and enables training using gradient descent." />
<meta property="og:description" content="The training of an Artificial Neural Network is dependent on an algorithm called Back-propagation, through which we calculate the effect each weight has on an error function. Each of these ‘effects’ is called a partial derivative, more specifically the partial derivative of the error function with respect to a given weight. A collection of partial derivatives (one for each weight), is called the gradient, and enables training using gradient descent." />
<link rel="canonical" href="http://localhost:4000/machine_learning/2020/07/19/training-feedforward-ann.html" />
<meta property="og:url" content="http://localhost:4000/machine_learning/2020/07/19/training-feedforward-ann.html" />
<meta property="og:site_name" content="Axtya Darius Barbano" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-07-19T13:19:41-04:00" />
<script type="application/ld+json">
{"description":"The training of an Artificial Neural Network is dependent on an algorithm called Back-propagation, through which we calculate the effect each weight has on an error function. Each of these ‘effects’ is called a partial derivative, more specifically the partial derivative of the error function with respect to a given weight. A collection of partial derivatives (one for each weight), is called the gradient, and enables training using gradient descent.","@type":"BlogPosting","url":"http://localhost:4000/machine_learning/2020/07/19/training-feedforward-ann.html","headline":"Training a feed-forward Neural Network with Back-propagation and Gradient Descent","datePublished":"2020-07-19T13:19:41-04:00","dateModified":"2020-07-19T13:19:41-04:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/machine_learning/2020/07/19/training-feedforward-ann.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Axtya Darius Barbano" /></head>
<!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-125635751-6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-125635751-6');
  </script>

  <!-- Mathjax Support -->
  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>

  <body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Axtya Darius Barbano</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <link rel="stylesheet" href="/assets/post.css">
<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Training a feed-forward Neural Network with Back-propagation and Gradient Descent</h1>
    <p class="post-meta"><time class="dt-published" datetime="2020-07-19T13:19:41-04:00" itemprop="datePublished">
        Jul 19, 2020
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>The training of an Artificial Neural Network is dependent on an algorithm called Back-propagation, through which we calculate the effect each weight has on an error function.
Each of these ‘effects’ is called a partial derivative, more specifically the <i>partial derivative</i> of the error function with respect to a given weight.
A collection of partial derivatives (one for each weight), is called the <i>gradient</i>, and enables training using <i>gradient descent</i>.</p>

<p>Here’s the example network we’ll be working with. It has just two input neurons, two hidden neurons, and one output.
We’ll also be using the Mean-Squared Error function to evaluate the correctness of each output.</p>

<p><img src="/images/ff_neural_net_1.jpg" alt="simple feed-forward neural network 1" /></p>

<h2 id="forward-propagation-in-a-feed-forward-network">Forward-propagation in a feed-forward network</h2>

<p>To show what a forward-pass would look like through this network, let’s give it a simple set of weights and the inputs 1 and 2.</p>

<p><img src="/images/ff_neural_net_2.jpg" alt="simple feed-forward neural network 2" /></p>

<p>As you can see, the final output of the network is \(1.9\). We got this because each neuron after the input layer computes its value
as the weighted sum of the values in the previous layer.
People generally have no problem understanding the forward-pass when learning about neural networks,
it’s Back-propagation (or backward-pass) that tends to be more difficult to comprehend.</p>

<h2 id="computing-error">Computing error</h2>

<p>The error (or cost) function gives a measure of how close the network’s output was to the desired output for any given input.
Suppose that, given the inputs of \(1\) and \(2\) that we used in the above example, we would like the network to output the value \(5\).
As mentioned earlier, we will be using the Mean-Squared Error function, defined as:</p>

\[E = \frac{1}{2} \sum_{i = 1}^{n} (o_i - y_i)^2\]

<p>Where \(o\) is a vector of \(n\) outputs and \(y\) is a vector of \(n\) desired values for each output. <br /> <br />
In our network, the output was \(1.9\) for the input \((0.1,0.2)\). Let’s suppose the desired value for this input was \(1.0\). Then, the error would be:</p>

\[E = \frac{1}{2}(o_1 - y_1)^2 = \frac{1}{2}(n_{3,1} - y_1)^2 = \frac{1}{2}(1.9 - 1.0)^2 = 0.405\]

<p>That’s all there is to it, the error for this training example is \(0.405\).</p>

<h2 id="back-propagation">Back-Propagation</h2>

<p>Now that we’ve discussed Forward-propagation and the procedure for computing error, let’s walk through back-propagation for this network.
Keep in mind that we computed our error using a desired output value of \(1\), so the gradient that will result from back-propagation will point the weights
in a direction that will adjust the network to produce this output. In fact, we will test the network again later after we perform gradient descent to see
if the output is closer to our desired value of \(1\).</p>

<p>Let’s begin by computing the partial derivative of the error function with respect to the value in the output neuron. In other words, we want to figure out
how the value in the output neuron affects the error (which is the value we want to minimize).</p>

\[\frac{\partial E}{\partial n_{3,1}} = \frac{\partial}{\partial n_{3,1}}(\frac{1}{2}(n_{3,1} - y_1)^2) = n_{3,1} - y_1\]

<p>The partial derivative here just requires a simple application of the power rule and chain rule. As you can see from the result,
the instantaneous rate of change of the error with respect to the output of the network is just \(n_{3,1} - y_1\).</p>

  </div><a class="u-url" href="/machine_learning/2020/07/19/training-feedforward-ann.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Axtya Darius Barbano</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Axtya Darius Barbano</li><li><a class="u-email" href="mailto:darius.barbano@gmail.com">darius.barbano@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/axtyax"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">axtyax</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Personal site of A. Darius Barbano</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
